{"cells":[{"cell_type":"code","source":["########################################################################################################\n","# Versión 1\n","# Generar modelos por estación y atributos seleccionados\n","# Modelo a utilizar LSTM\n","# SEGUN LOS \"n\" DIAS PREVIOS (VENTANA DE \"n\" VALORES), PREDICE los \"ventSalida\" DÍAS SIGUIENTES\n","# del atributo seleccionado\n","# Guarda los modelos generados con el formato: Modelo_ + COD_ESTACION + ATRIBUTO   ej.: Modelo_46250048_PM10.h5\n","########################################################################################################"],"metadata":{"id":"cgcvM7yML62d","executionInfo":{"status":"ok","timestamp":1654098590972,"user_tz":-120,"elapsed":324,"user":{"displayName":"Santiago Cámara","userId":"17493840598039060762"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Parámetros del algoritmo\n","\n","# Definir el número de días de la ventana de entrada (días previos a considerar para la predicción)\n","n = 30\n","\n","# Definir el número de días a predecir por cada ventana de entrada\n","ventSalida = 1\n","\n","# seleccionar lista de atributos a generar modelo de predicción\n","atributos = ['PM10', 'O3']\n","\n","# Definir número de epoch máximo para entrenamiento\n","max_epochs = 50"],"metadata":{"id":"pJbq2i77Oa5V","executionInfo":{"status":"ok","timestamp":1654098591173,"user_tz":-120,"elapsed":8,"user":{"displayName":"Santiago Cámara","userId":"17493840598039060762"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"id":"c8qUlIpcgvE0","executionInfo":{"status":"ok","timestamp":1654098591174,"user_tz":-120,"elapsed":7,"user":{"displayName":"Santiago Cámara","userId":"17493840598039060762"}}},"outputs":[],"source":["# librerías necesarias\n","\n","import pandas as pd\n","import numpy as np\n","import plotly \n","import plotly.express as px\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","from keras.wrappers.scikit_learn import KerasClassifier\n","from keras.models import Sequential\n","from keras.layers import Input, Dense, LSTM, Dropout\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import GridSearchCV\n","\n","np.random.seed(100)\n","tf.random.set_seed(100)\n","\n","# Configuración general de las figuras que representaremos\n","import matplotlib as mpl\n","mpl.rcParams['figure.figsize'] = (20, 10)\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\") # specify to ignore warning messages"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CFmKywTwWK3s","executionInfo":{"status":"ok","timestamp":1654098594413,"user_tz":-120,"elapsed":3245,"user":{"displayName":"Santiago Cámara","userId":"17493840598039060762"}},"outputId":"9d6ff63f-5914-4c92-cc09-b0aa81e5a889"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/datoscsv/'\n","dfCompleto = pd.read_csv(path + 'Valencia_SeleccionDatos_ParaModelar_v1.csv', sep=';', index_col='fecha', parse_dates=['fecha'])\n","\n","# Recuperar la fecha como campo\n","dfCompleto.insert(0, 'FECHA', dfCompleto.index.strftime('%Y-%m-%d'))\n"],"metadata":{"id":"qKaoXKCGXt4r","executionInfo":{"status":"ok","timestamp":1654098594841,"user_tz":-120,"elapsed":431,"user":{"displayName":"Santiago Cámara","userId":"17493840598039060762"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":14,"metadata":{"id":"fmWZ6RXHj2a9","executionInfo":{"status":"ok","timestamp":1654098594842,"user_tz":-120,"elapsed":5,"user":{"displayName":"Santiago Cámara","userId":"17493840598039060762"}}},"outputs":[],"source":["def procesar_atributo(atributo, cod_esta, nom_esta, df):\n","  print(f'Procesando atributo: {atributo}')\n","  print()\n","  # eliminar filas sin el atributo para probar\n","  df = df[df[atributo].notnull()]\n","  # Vamos a coger hasta el 2018 para entrenamiento y los datos del 2019 para testear\n","  # Para poder introducir los datos en una red neuronal hay que transformarlos, ya que solo funciona con arrays\n","  df2 = df[[atributo]]\n","  # print('df2:', df2.describe().transpose())\n","  train = df2[:'2018']    # guardar desde fecha más antigua hasta final de 2018\n","  test = train.iloc[-n:]  # guardar los \"n\" últimos dias del 2018\n","  test = test.append(df2['2019'], ignore_index=False) # añadir a los \"n\" días del 2018 + el 2019 completo\n","\n","  train[atributo].plot(legend=True)\n","  test[atributo].plot(legend=True)\n","  plt.legend(['Train (inicio-2018)', 'Test (2019)'])\n","  plt.title(nom_esta + ': ' + atributo )\n","  plt.show()\n","  print()\n","  \n","  # normalizar datos\n","  sc = MinMaxScaler(feature_range=(0,1))\n","  train_scaled = sc.fit_transform(train)\n","\n","  # Preparar ventana de entrada\n","  # Hay que crear una estructura de datos para indicarle a la red lo que tiene que recordar \n","  # y poder hacer una predección en base a datos anteriores\n","  X_train = []\n","  y_train = []\n","\n","  # Estructura de datos con \"n\" pasos y uno o mas valores de salida\n","  # \"n\" timesteps significa que para un día dado la red es capaz de mirar los \"n\" días anteriores:\n","  # \"n\" valores anteriores al día actual y en base a esa información predice los \"ventSalida\" dias futuros \n","  for i in range(n,len(train_scaled)):\n","      # X: para cada día hacemos un bloque correspondiente al valor de los \"n\" días anteriores\n","      X_train.append(train_scaled[i-n:i,0])\n","      # Y: el valor del día\n","      y_train.append(train_scaled[i,0])\n","\n","  # Tenemos una matriz donde cada fila contiene \"n\" columnas correspondientes a los \"n\" días anteriores\n","  X_train, y_train = np.array(X_train), np.array(y_train)\n","\n","  # Reshape X_train para que se ajuste al modelo en Keras\n","  # añadiendo una nueva dimensión \n","  X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n","\n","  # Modelo LSTM\n","  # Creamos un modelo secuencial al que iremos añadiendo capas, que pueden ser:\n","\n","  # Dense: Capa de neuronas artificiales\n","  # LSTM: Capa de neuronas con capacidad de memoria\n","  # Dropout: Es una técnica de regularización que elimina conexiones neuronales para evitar el sobreajuste, de esta forma se evita\n","  #          que el algoritmo memorice los datos en vez de aprender de ellos.\n","  # Antes de crear la Red LSTM debemos reajustar los sets que acabamos de obtener, para indicar que cada ejemplo de entrenamiento a\n","  #          la entrada del modelo será un vector de \"n\" x 1.\n","  # Para el caso de cada una de las salidas (almacenadas en y_train) debemos simplemente especificar que su tamaño, que será igual\n","  #           al número de valores predichos por la red neuronal.\n","  # Definimos el tamaño de los datos de entrada y de salida\n","  dim_entrada = (X_train.shape[1],1)\n","  dim_salida = ventSalida\n","\n","  modelo = Sequential()\n","  # Añadimos la primera capa y especificamos el número de neuromas (50) y el tamaño de cada dato de entrada\n","  modelo.add(LSTM (units=50, return_sequences=True, input_shape=dim_entrada))\n","  modelo.add(Dropout(0.2))\n","  # Añadimos la segunda capa\n","  modelo.add(LSTM (units=50, return_sequences=True))\n","  modelo.add(Dropout(0.2))\n","  # Añadimos la tercera capa\n","  modelo.add(LSTM (units=50, return_sequences=True))\n","  modelo.add(Dropout(0.2))\n","  # Añadimos la cuarta capa\n","  modelo.add(LSTM (units=50))\n","  modelo.add(Dropout(0.2))\n","  # Añadimos la capa de salida (Dense) y especificamos que el campo de salida es igual a dim_salida\n","  modelo.add(Dense(units=dim_salida))\n","\n","  # Compilación\n","  # El optimizador se encarga de optimizar y actualizar los pesos de la red. Usaremos el algoritmo adam que es el que mejores resultados nos aporta.\n","  # loss: Cuando la red hace una predicción coge el valor real y lo compará con la predicción. Para calcular el error de predicción, se utilizará el método error cuadrático medio.\n","  \n","  # Compilamos el modelo definiendo la función de error (loss) y el método que se usa para minimizarla (optimizer)\n","  modelo.compile(optimizer='adam', loss='mean_squared_error')\n","\n","  # Entrenamiento\n","  # epochs: Especificamos las veces que la red propagara el error hacia atrás para aprender y hacer mejores predicciones. Es la cantidad de veces que la red realizará una predicción y propagara hacia atrás el error, para aprender de él y hacer mejores predicciones\n","  # batch_size: Para este proceso de predicción, corrección y propagación hacia atrás, no lo haremos con un solo bloque de datos, si no de 32. Si no el entrenamiento sería demasiado pesado para la red, el tener que actualizar los pesos a cada dato. Vamos actualizando los pesos con bloques de datos.\n","\n","  # Entrenamos el modelo con max_epochs (iteraciones) y batch_size=32 (lotes de 32 datos)\n","\n","  callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n","\n","  modelo.fit(X_train,y_train,epochs=max_epochs, batch_size=32, callbacks=[callback])\n","\n","  # Predicciones\n","  # Hay que preparar los datos como en el caso de train, esto es, hacemos bloques de \"n\" días para predecir el valor de la acción en el siguiente día.\n","  test_scaled = sc.fit_transform(test)\n","\n","  X_test = []\n","  for i in range(n,len(test_scaled)):\n","      X_test.append(test_scaled[i-n:i,0])\n","\n","  X_test = np.array(X_test)\n","  X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n","\n","  # Realizamos la prediccion y aplicamos normalización inversa para que esté en escala real\n","  prediccion = modelo.predict(X_test)\n","  prediccion = sc.inverse_transform(prediccion)\n","\n","  # Visualización del resultado\n","  resultados = test.tail(len(test)-n)\n","  resultados['Prediccion'] = prediccion\n","\n","  # print(resultados)\n","\n","  plt.plot(train[atributo], color='green')\n","  plt.plot(resultados[atributo], color='red', label='Valor real de ' + atributo)\n","  plt.plot(resultados['Prediccion'], color='blue', label='Predicción de ' + atributo)\n","  plt.xlabel('Tiempo')\n","  plt.ylabel('Valor de ' + atributo)\n","  plt.legend()\n","  plt.title(nom_esta + ': ' + atributo )\n","  plt.show()\n","  print()\n","\n","  plt.plot(resultados[atributo], color='red', label='Valor real de ' + atributo)\n","  plt.plot(resultados['Prediccion'], color='blue', label='Predicción de ' + atributo)\n","  plt.xlabel('Tiempo')\n","  plt.ylabel('Valor de ' + atributo)\n","  plt.legend()\n","  plt.title(nom_esta + ': ' + atributo )\n","  plt.show()\n","  print()\n","\n","  from sklearn.metrics import mean_absolute_error\n","  from sklearn.metrics import mean_squared_error\n","  from math import sqrt\n","\n","  # ratios de la predicción\n","  mae = mean_absolute_error(resultados['Prediccion'], resultados[atributo])\n","  rmse = sqrt(mean_squared_error(resultados['Prediccion'], resultados[atributo]))\n","  \n","  print('Modelo:' , cod_esta, '-', atributo )\n","  print('Error cuadrático medio: %.3f' % rmse)\n","  print('  Error absoluto medio: %.3f' % mae)\n","  print('========================================')\n","  print()\n","\n","  # Guardar el Modelo\n","  pathModelos = '/content/drive/MyDrive/modelosGuardados/'\n","  modelo.save(pathModelos + 'Modelo_' + str(cod_esta) + '_' + atributo + '.h5')\n","\n","  # Recrea exactamente el mismo modelo solo desde el archivo:\n","  # new_model = keras.models.load_model('nombremodelo.h5')\n","\n","    "]},{"cell_type":"code","source":["def procesar_estacion(cod_esta, nom_esta, df):\n","  print(f'Procesando estacion: {cod_esta} - {nom_esta}')\n","  print()\n","  # seleccionar los datos de la estacion a procesar\n","  df = df[df['COD_ESTACION']==cod_esta]\n","  # print(df)\n","  # procesar los atributos de la estación\n","  for atrib in atributos:\n","    procesar_atributo(atrib, cod_esta, nom_esta, df)"],"metadata":{"id":"OON96WVbwfVK","executionInfo":{"status":"ok","timestamp":1654098595170,"user_tz":-120,"elapsed":332,"user":{"displayName":"Santiago Cámara","userId":"17493840598039060762"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Proceso principal\n","\n","# print(dfCompleto['COD_ESTACION'].unique())\n","# print(dfCompleto['NOM_ESTACION'].unique())\n","cod_estaciones = dfCompleto['COD_ESTACION'].unique()\n","nom_estaciones = dfCompleto['NOM_ESTACION'].unique()\n","for cod_esta, nom_esta in zip(cod_estaciones, nom_estaciones):\n","  procesar_estacion(cod_esta, nom_esta, dfCompleto)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1LX5m_Ap2JuSreQAAswB-kyKT8kgiOcd9"},"id":"MZFXQHAWwfNF","executionInfo":{"status":"ok","timestamp":1654100616291,"user_tz":-120,"elapsed":2021125,"user":{"displayName":"Santiago Cámara","userId":"17493840598039060762"}},"outputId":"4dcff0ff-b34b-4608-9873-106ec3bbe932"},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}],"metadata":{"colab":{"name":"030_generar_modelos_porEstacion.ipynb","provenance":[],"collapsed_sections":[]},"interpreter":{"hash":"997767116ae04da281f657ba1ef80941fde8037360d002d2f6ec7e11d1f836d2"},"kernelspec":{"display_name":"Python 3.8.8 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":0}